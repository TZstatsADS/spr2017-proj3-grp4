library(DMwR)
library(nnet)
feature <- read.csv("./data/sift_features.csv", header = T)
feature<- t(feature)
data <- data.frame(feature)
data$y <- c(rep(1,1000), rep(0,1000))
data$y <- as.factor(data$y)
n <- nrow(data)
set.seed(2)
samp <- sample(1:n,n/5)
traindata<- data[-samp, ]
testdata<- data[samp, ]
model.nnet <- nnet(y ~ ., data = traindata, linout = F,
size = 2, decay = 0.01, maxit = 200,
trace = F, MaxNWts = 20000)
pre.nnet <- predict(model.nnet, traindata, type = "class")
table(pre.nnet, traindata$y)
test.nnet <- predict(model.nnet, testdata, type = "class")
table(test.nnet, testdata$y)
library(DMwR)
library(nnet)
feature <- read.csv(".../data/sift_features.csv", header = T)
feature<- t(feature)
data <- data.frame(feature)
data$y <- c(rep(1,1000), rep(0,1000))
data$y <- as.factor(data$y)
n <- nrow(data)
set.seed(2)
samp <- sample(1:n,n/5)
traindata<- data[-samp, ]
testdata<- data[samp, ]
model.nnet <- nnet(y ~ ., data = traindata, linout = F,
size = 2, decay = 0.01, maxit = 200,
trace = F, MaxNWts = 20000)
pre.nnet <- predict(model.nnet, traindata, type = "class")
table(pre.nnet, traindata$y)
test.nnet <- predict(model.nnet, testdata, type = "class")
table(test.nnet, testdata$y)
library(DMwR)
library(nnet)
feature <- read.csv(".../data/sift_features.csv", header = T)
feature <- read.csv("../data/sift_features.csv", header = T)
install.packages("DMwR")
library(DMwR)
library(nnet)
feature <- read.csv("../data/sift_features.csv", header = T)
feature<- t(feature)
data <- data.frame(feature)
data$y <- c(rep(0,1000), rep(1,1000))
data$y <- as.factor(data$y)
n <- nrow(data)
set.seed(2)
samp <- sample(1:n,n/5)
traindata<- data[-samp, ]
testdata<- data[samp, ]
model.nnet <- nnet(y ~ ., data = traindata, linout = F,
size = 2, decay = 0.01, maxit = 200,
trace = F, MaxNWts = 20000)
pre.nnet <- predict(model.nnet, traindata, type = "class")
table(pre.nnet, traindata$y)
test.nnet <- predict(model.nnet, testdata, type = "class")
table(test.nnet, testdata$y)
############ Random Forest######################
# First tune random forest model, tune parameter 'mtry'
library(randomForest)
bestmtry <- tuneRF(y= traindata$y, x= traindata, stepFactor=1.5, improve=1e-5, ntree=600)
best.mtry <- bestmtry[,1][which.min(bestmtry[,2])]
model.rf <- randomForest(y ~ ., data = traindata, ntree=600, mtry=best.mtry, importance=T)
pre.rf <- predict(model.rf, traindata, type = "class")
table(pre.rf, traindata$y)
test.rf <- predict(model.rf, testdata, type = "class")
table(test.rf, testdata$y)
install.packages("randomForest")
library(randomForest)
bestmtry <- tuneRF(y= traindata$y, x= traindata, stepFactor=1.5, improve=1e-5, ntree=600)
index=sample(1:2000,400,replace = F)
all=data.frame(cbind(label,t(sift.features)))
sift.features=read.csv("../data/sift_features.csv", header = T)
label=read.csv("../data/labels.csv")
index=sample(1:2000,400,replace = F)
all=data.frame(cbind(label,t(sift.features)))
colnames(all)[1]="y"
test=all[index,]
train=all[-index,]
train <- data.matrix(train)
train_x <- t(train[, -1])
train_y <- train[, 1]
train_array <- train_x
test_x <- t(test[, -1])
test_y <- test[, 1]
test_array <- test_x
data <- mx.symbol.Variable('data')
library(mxnet)
data <- mx.symbol.Variable('data')
conv_1 <- mx.symbol.Convolution(data = data, kernel = c(5, 5), num_filter = 20)
tanh_1 <- mx.symbol.Activation(data = conv_1, act_type = "tanh")
pool_1 <- mx.symbol.Pooling(data = tanh_1, pool_type = "max", kernel = c(2, 2), stride = c(2, 2))
# 2nd convolutional layer
conv_2 <- mx.symbol.Convolution(data = pool_1, kernel = c(5, 5), num_filter = 50)
tanh_2 <- mx.symbol.Activation(data = conv_2, act_type = "tanh")
pool_2 <- mx.symbol.Pooling(data=tanh_2, pool_type = "max", kernel = c(2, 2), stride = c(2, 2))
# 1st fully connected layer
flatten <- mx.symbol.Flatten(data = pool_2)
fc_1 <- mx.symbol.FullyConnected(data = flatten, num_hidden = 500)
tanh_3 <- mx.symbol.Activation(data = fc_1, act_type = "tanh")
# 2nd fully connected layer
fc_2 <- mx.symbol.FullyConnected(data = tanh_3, num_hidden = 40)
# Output. Softmax output since we'd like to get some probabilities.
NN_model <- mx.symbol.SoftmaxOutput(data = fc_2)
devices <- mx.cpu()
model <- mx.model.FeedForward.create(NN_model,
X = train_array,
y = train_y,
ctx = devices,
num.round = 480,
array.batch.size = 40,
learning.rate = 0.01,
momentum = 0.9,
eval.metric = mx.metric.accuracy,
epoch.end.callback = mx.callback.log.train.metric(100))
NN_model <- mx.symbol.SoftmaxOutput(data = fc_2)
model <- mx.model.FeedForward.create(NN_model,
X = train_array,
y = train_y,
ctx = devices,
num.round = 480,
array.batch.size = 40,
learning.rate = 0.01,
momentum = 0.9,
eval.metric = mx.metric.accuracy,
epoch.end.callback = mx.callback.log.train.metric(100))
dim(train_array) <- c(28, 28, 1, ncol(train_x))
dim(train_array) <- c(1400, 1400, 1, ncol(train_x))
model <- mx.model.FeedForward.create(NN_model,
X = train_array,
y = train_y,
ctx = devices,
num.round = 480,
array.batch.size = 40,
learning.rate = 0.01,
momentum = 0.9,
eval.metric = mx.metric.accuracy,
epoch.end.callback = mx.callback.log.train.metric(100))
test_array
test_x
train
View(train_x)
View(train_array)
train_y <- train[, 1]
train_y
train <- data.matrix(train)
train_x <- t(train[, -1])
train_y <- train[, 1]
train_array <- train_x
test_x <- t(test[, -1])
test_y <- test[, 1]
test_array <- test_x
View(train_array)
model <- mx.model.FeedForward.create(NN_model,
X = train_array,
y = train_y,
ctx = devices,
num.round = 480,
array.batch.size = 40,
learning.rate = 0.01,
momentum = 0.9,
eval.metric = mx.metric.accuracy,
epoch.end.callback = mx.callback.log.train.metric(100))
model <- mx.mlp(train_x, train_y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
model <- mx.mlp(train_x, train_y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=480, array.batch.size=40, learning.rate=0.03, momentum=0.9,
eval.metric=mx.metric.accuracy)
predict(model,test_x)
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10)
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm")
devices <- mx.cpu()
model <- mx.model.FeedForward.create(softmax, X=train_x, y=train_y,
ctx=devices, num.round=100, array.batch.size=100,
learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
initializer=mx.init.uniform(0.07),
epoch.end.callback=mx.callback.log.train.metric(100))
sift.features=read.csv("../data/sift_features.csv", header = T)
label=read.csv("../data/labels.csv")
source("../lib/train.baseline.r")
source("../lib/test.baseline.r")
source("../lib/train.bp.r")
source("../lib/test.bp.r")
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(1:200,1001:1200)
colnames(all)[1]="y"
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
bp.model=train.bp(train)
bp.pre=test.bp(bp.model,test.x)
bp.pre=test.bp(bp.model,test.x)
predict(bp.model, test.x, type = "class")
all=data.frame(cbind(as.factor(label),t(sift.features)))
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(1:200,1001:1200)
colnames(all)[1]="y"
all$y=as.factor(all$y)
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
bp.model=train.bp(train)
bp.pre=test.bp(bp.model,test.x)
predict(bp.model, test.x, type = "class")
bp.pre=test.bp(bp.model,test.x)
table(bp.pre,test$y)
source("../lib/train.BPrf.r")
source("../lib/test.BPrf.r")
source("../lib/train.BPrf.r")
rf.model=train.rf(train)
model.rf <- randomForest(y ~ ., data = traindata, ntree=600, mtry=235, importance=T)
model.rf <- randomForest(y ~ ., data = train, ntree=600, mtry=235, importance=T)
test.rf(model.rf,test.x)
rf.pre=test.rf(model.rf,test.x)
table(rf.pre,test$y)
sum(baseline.pre,rf.pre,bp.pre)
rowSums(cbind(baseline.pre,rf.pre,bp.pre))
rowSums(as.numeric(cbind(baseline.pre,rf.pre,bp.pre)))
rowSums(as.numeric(c(baseline.pre,rf.pre,bp.pre)))
baseline.pre+rf.pre
as.numeric(baseline.pre)+as.numeric(rf.pre)
as.numeric(baseline.pre)+as.numeric(rf.pre)+as.numeric(bp.pre)
baseline.pre
rf.pre
bp.pre
bp.pre
as.numeric(baseline.pre)+as.numeric(rf.pre)+as.numeric(bp.pre)
as.numeric(baseline.pre)
as.numeric(rf.pre)
rf.pre
(as.numeric(baseline.pre)+as.numeric(rf.pre)+as.numeric(bp.pre))>3
pre=ifelse((as.numeric(baseline.pre)+as.numeric(rf.pre)+as.numeric(bp.pre))>3,1,0)
table(pre,test$y)
pre=ifelse((as.numeric(baseline.pre)+as.numeric(rf.pre)-1+as.numeric(bp.pre))>2,1,0)
table(pre,test$y)
model.rf <- randomForest(y ~ ., data = train, ntree=600, mtry=167, importance=T)
library(mxnet)
train.x=data.matrix(train[,-1])
train.y=train[,1]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
library(mxnet)
train.x=t(data.matrix(train[,-1]))
train.y=train[,1]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
library(mxnet)
train.x=t(data.matrix(train[,-1]))
train.y=train[,1]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
library(mxnet)
train.x=data.matrix(train[,-1])
train.y=train[,1]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
library(mxnet)
train.x=data.matrix(train[,-1])
train.y=train[,1]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
knn_result=knn(train.x,test.x,cl=train.y,k=1,prob = T)
library(FNN)
install.packages("FNN")
library(FNN)
knn_result=knn(train.x,test.x,cl=train.y,k=1,prob = T)
knn_result
table(knn_result,test$y)
(as.numeric(baseline.pre)+as.numeric(rf.pre)-1+as.numeric(bp.pre)+as.numeric(knn_result))
as.numeric(knn_result))
as.numeric(knn_result)
as.numeric(as.character(knn_result))
(as.numeric(baseline.pre)+as.numeric(as.character(rf.pre))+as.numeric(bp.pre)+as.numeric(as.character(knn_result)))
pre=(as.numeric(baseline.pre)+as.numeric(as.character(rf.pre))+as.numeric(bp.pre)+as.numeric(as.character(knn_result)))
pre>2
table(pre>2,test$y)
table(pre>=2,test$y)
pre=(as.numeric(baseline.pre)+as.numeric(as.character(rf.pre))+as.numeric(bp.pre))
table(pre>=2,test$y)
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(201:400,1201:1400)
colnames(all)[1]="y"
all$y=as.factor(all$y)
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
baseline.model$n
sift.features=read.csv("../data/sift_features.csv", header = T)
label=read.csv("../data/labels.csv")
source("../lib/train.baseline.r")
source("../lib/test.baseline.r")
source("../lib/train.BPrf.r")
source("../lib/test.BPrf.r")
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(201:400,1201:1400)
colnames(all)[1]="y"
all$y=as.factor(all$y)
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
source("../lib/train.baseline.r")
source("../lib/test.baseline.r")
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
source("../lib/train.baseline.r")
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
baseline.pre
gbm1=gbm(y~.,data=train.data,dist="adaboost",n.tree = 200,shrinkage=0.1)
gbm1=gbm(y~.,data=train,dist="adaboost",n.tree = 200,shrinkage=0.1)
gbm1
gbm.perf(gbm1
)
gbm1=gbm(y~.,data=train,dist="adaboost",n.tree =40,shrinkage=0.1)
predict(gbm1,test.x,n.tree=20)
library(gbm)
library(caret)
n=gbm.perf(gbm1)
sift.features=read.csv("../data/sift_features.csv", header = T)
label=read.csv("../data/labels.csv")
source("../lib/train.baseline.r")
source("../lib/test.baseline.r")
source("../lib/train.BPrf.r")
source("../lib/test.BPrf.r")
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(201:400,1201:1400)
colnames(all)[1]="y"
all$y=as.factor(all$y)
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
bp.model=train.bp(train)
sift.features=read.csv("../data/sift_features.csv", header = T)
sift.features=read.csv("../data/sift_features.csv", header = T)
label=read.csv("../data/labels.csv")
source("../lib/train.baseline.r")
source("../lib/test.baseline.r")
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(201:400,1201:1400)
colnames(all)[1]="y"
all$y=as.factor(all$y)
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
source("../lib/train.baseline.r")
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
all=data.frame(cbind(label,t(sift.features)))
test.index1=c(201:400,1201:1400)
colnames(all)[1]="y"
#all$y=as.factor(all$y)
test=all[test.index1,]
test.x=test[,-1]
train=all[-test.index1,]
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
source("../lib/train.baseline.r")
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
