---
title: "Puppy or Fried Chicken?"
output: html_notebook
---


#Summary:
In this project, we created a classifier for grey images of puppies versus images of fried chickens. We tried different features (SIFT, LBP) and different classifiers (GBM, BP Neural Networks, SVM, Random Forest and Majority Vote). When pursuing low error rate, we also keep an eye on processing time.

##install packages
```{r}
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

```

##read data
```{r,warning=F}
sift.features=read.csv("../data/sift_features.csv", header = T)
lbp=read.csv("../data/lbp.csv", header = F)
label=read.csv("../data/labels.csv")

source("../lib/train.r")
source("../lib/test.r")
```

##train and validate set 
```{r}
all=data.frame(cbind(label,t(sift.features)))
test.index=sample(1:2000,400,replace=F)
colnames(all)[1]="y"
test.sift=all[test.index,]
test.x.sift=test.sift[,-1]
train.sift=all[-test.index,]
```


#Baseline: GBM + SIFT
Tune parameters:  n.trees=133, shrinkage=0.1
```{r,warnings=F}
start.time <- Sys.time()
baseline.model=train.baseline(train.sift)
baseline.pre=test.baseline(baseline.model,test.x.sift)
table(baseline.pre,test.sift$y)
end.time <- Sys.time()
baseline.time=end.time - start.time

```
#Other models + SIFT
We tried to apply other models on 5000-dimensional SIFT features. When the accuracy rate increased to ~80%, the processing time increased dramatically. So we used PCA to reduce the dimensional of SIFT features. However, when the dimension decreased to 500, the results of models didn’t seem satisfying. Thus, we started to explore other features.

#Local Binary Patterns (LBP)
•	Divide the examined window into cells (e.g. 16x16 pixels for each cell).
•	For each pixel in a cell, compare the pixel to each of its 8 neighbors
•	Where the center pixel's value is greater than the neighbor's value, write "0". Otherwise, write "1". This gives an 8-digit binary number.
•	Compute the histogram over the cell. This histogram can be seen as a 256-dimensional feature vector.
•	Optionally normalize the histogram to 59-dimensional feature vector.
•	Concatenate histograms of all cells. This gives a feature vector for the entire window.
Then we extracted LBP features in MATLAB. The processing time of 2000 images is 210s. The column dimension of the result feature matrix is 59.


```{r}
new.data=data.frame(cbind(label,lbp))
colnames(new.data)[1]="y"
test=new.data[test.index,]
test.x=test[,-1]
train=new.data[-test.index,]
```



#GBM + LBP
```{r}
start.time <- Sys.time()
baseline.model=train.baseline(train)
baseline.pre=test.baseline(baseline.model,test.x)
table(baseline.pre,test$y)
end.time <- Sys.time()
end.time - start.time
```

#BP Neural Networks + LBP
```{r}
start.time <- Sys.time()
bp.model=train.bp(train)
bp.pre=test.bp(bp.model,test.x)
table(bp.pre,test$y)
end.time <- Sys.time()
end.time - start.time
```

#Random Forest + LBP
```{r}
start.time <- Sys.time()
rf.model=train.rf(train)
rf.pre=test.rf(rf.model,test.x)
table(rf.pre,test$y)
end.time <- Sys.time()
end.time - start.time
```


#SVM + LBP
```{r}
start.time <- Sys.time()
source("../lib/train.r")
svm.model=train.svm(train)
svm.pre=test.svm(svm.model,test.x)
table(svm.pre,test$y)
end.time <- Sys.time()
end.time - start.time

```

#Logistic + LBP
```{r}
start.time <- Sys.time()
log.model=train.log(train)
log.pre=test.log(log.model, test.x)
table(log.pre, test$y)
end.time <- Sys.time()
end.time - start.time
```


#Majority Vote (NN, SVM, Log) + LBP
```{r}
pre=(as.numeric(as.character(bp.pre))+as.numeric(as.character(log.pre))+as.numeric(as.character(svm.pre)))
pre=ifelse(pre>=2,1,0)
table(pre,test$y)
```

#Cross validation error rate
```{r}
source("../lib/cross_validation.R")
cv.error=cv.function(train, 5)
cv.error
```


#Final Model
We choose Majority Vote as our final model. Since training time of each model is very short, time won't be a problem for majority vote. Although we sacrifice little accuracy, We can get a more robust model.
